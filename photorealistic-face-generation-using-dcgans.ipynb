{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05865837",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/FarzadNekouee/DCGAN-Photorealistic-Face-Generator/blob/master/image.png?raw=true\" width=\"1800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb321e8",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #d5d5ed; font-size:125%; text-align:left\">\n",
    "\n",
    "We're all probably familiar with the intriguing website [This Person Does Not Exist](https://thispersondoesnotexist.com/), showcasing the power of modern machine learning in generating photorealistic, yet entirely fictitious, human faces. Inspired by this, our project aims to design a model capable of producing photorealistic face images that, although resembling real people, are completely synthesized. Due to resource constraints, we will be focusing on generating images at a lower resolution of __64x64__ pixels. In this endeavor, we're utilizing a subset of the [CelebFaces Attributes (CelebA) Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html), specifically the first 50,000 images. This subset is conveniently available on [Kaggle](https://www.kaggle.com/datasets/farzadnekouei/50k-celebrity-faces-image-dataset). Our chosen method for image synthesis is the __Deep Convolutional Generative Adversarial Networks__ (__DCGANs__). Prior to training this DCGAN model, our dataset will undergo a rigorous preprocessing phase to ensure optimal results. The ultimate objective is to train our DCGAN to synthesize new, lifelike faces that evoke the essence of the celebrities in our training set, albeit in a fabricated manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b25c78",
   "metadata": {},
   "source": [
    "<a id=\"contents_tabel\"></a>    \n",
    "<div style=\"border-radius:10px; padding: 15px; background-color: #d5d5ed; font-size:120%; text-align:left\">\n",
    "\n",
    "<h2 align=\"left\"><font color=#22199e>Table of Contents: </font></h2>\n",
    "    \n",
    "* <a href=\"#import\" style=\"color: #22199e; text-decoration: none;\">Step 1 | Import Necessary Libraries</a>\n",
    "* <a href=\"#preprocess\" style=\"color: #22199e; text-decoration: none;\">Step 2 | Loading and Preprocessing the Dataset</a>\n",
    "* <a href=\"#architecture\" style=\"color: #22199e; text-decoration: none;\">Step 3 | Define DCGAN Architecture</a>\n",
    "* <a href=\"#data_preparation\" style=\"color: #22199e; text-decoration: none;\">Step 4 | Prepare Data for Training DCGAN</a>\n",
    "* <a href=\"#train\" style=\"color: #22199e; text-decoration: none;\">Step 5 | Train DCGAN & Monitor Progress</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69742a72",
   "metadata": {},
   "source": [
    "<h2 align=\"left\"><font color='#22199e'>Let's get started!</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf50f93",
   "metadata": {},
   "source": [
    "<a id=\"import\"></a>\n",
    "# <p style=\"background-color:#22199e; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 1 | Import Necessary Libraries</p>\n",
    "\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e9d47",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:12px; padding: 20px; background-color: #d5d5ed; font-size:115%; text-align:left\">\n",
    "    \n",
    "First of all, let's import necessary libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17b5a85f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:40:50.016761Z",
     "iopub.status.busy": "2023-08-27T05:40:50.016206Z",
     "iopub.status.idle": "2023-08-27T05:40:58.393543Z",
     "shell.execute_reply": "2023-08-27T05:40:58.392535Z",
     "shell.execute_reply.started": "2023-08-27T05:40:50.016722Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17428/4038529360.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2DTranspose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from numpy import zeros, ones\n",
    "from numpy.random import randn, randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Conv2DTranspose, LeakyReLU\n",
    "from keras.layers import BatchNormalization, Dropout, Reshape, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e351be",
   "metadata": {},
   "source": [
    "<a id=\"preprocess\"></a>\n",
    "# <p style=\"background-color:#22199e; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 2 | Loading and Preprocessing the Dataset</p>\n",
    "\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e965e1",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #d5d5ed; font-size:115%; text-align:left\">\n",
    "\n",
    "The first step in our pipeline involves preparing our data for the DCGAN model. For this project, we're using a subset of the __CelebFaces Attributes (CelebA) Dataset__, which is available on [Kaggle](https://www.kaggle.com/datasets/farzadnekouei/50k-celebrity-faces-image-dataset).\n",
    "\n",
    "To ensure our model receives optimal data, we perform the following preprocessing steps on each image:\n",
    "    \n",
    "* __Read Image__: We start by obtaining the path to every image within the dataset directory.\n",
    "    \n",
    "* __Crop__: Each image gets cropped, removing 20 pixels from the top and bottom, resulting in a square image.\n",
    "    \n",
    "* __Resize__: Given the computational constraints and our DCGAN's architecture, we resize the images to a resolution of 64x64 pixels.\n",
    "    \n",
    "* __Normalize__: Images are converted to numpy arrays and then normalized to the range [-1, 1] to facilitate the neural network's training process.\n",
    "\n",
    "By the end of this preprocessing phase, our dataset will be a numpy array with the shape indicating the number of images and the dimensions of each image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd8de54",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #d5d5ed; font-size:115%; text-align:left\">\n",
    "<h3 align=\"left\"><font color=royalblue>üîî Note:</font></h3>   \n",
    "    \n",
    "Given constraints on computational resources and the desire to streamline training times, we have opted to train our model on a subset of 20,000 images. However, if resources permit, training with a larger dataset can potentially enhance the model's performance and yield improved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a8e9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:40:58.396077Z",
     "iopub.status.busy": "2023-08-27T05:40:58.395335Z",
     "iopub.status.idle": "2023-08-27T05:44:18.408488Z",
     "shell.execute_reply": "2023-08-27T05:44:18.407301Z",
     "shell.execute_reply.started": "2023-08-27T05:40:58.396040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the directory of your images on Kaggle\n",
    "dataset_dir = \"/kaggle/input/50k-celebrity-faces-image-dataset/Celebrity_Faces_Dataset\" \n",
    "\n",
    "# Get a list of all image paths in the directory\n",
    "image_paths = glob.glob(os.path.join(dataset_dir, '*.jpg'))\n",
    "\n",
    "# Considering only the first 20,000 images\n",
    "image_paths = image_paths[:20000]\n",
    "\n",
    "# Create a function to open, crop and resize images\n",
    "def load_and_preprocess_real_images(image_path, target_size=(64, 64)):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "    # Crop 20 pixels from the top and bottom to make it square\n",
    "    img = img.crop((0, 20, 178, 198))\n",
    "    # Resize the image\n",
    "    img = img.resize(target_size)\n",
    "    # Convert to numpy array and scale to [-1, 1]\n",
    "    img = np.array(img)/127.5 - 1\n",
    "    return img\n",
    "\n",
    "# Open, crop and resize all images\n",
    "dataset = np.array([load_and_preprocess_real_images(img_path) for img_path in image_paths])\n",
    "\n",
    "# Print dataset shape\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e28946",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #d5d5ed; font-size:115%; text-align:left\">\n",
    "\n",
    "Having preprocessed our dataset, it's now time to visually examine the images that we'll be using to train our DCGAN model. Here, we're displaying a small sample of the processed images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc80757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:44:18.410314Z",
     "iopub.status.busy": "2023-08-27T05:44:18.409884Z",
     "iopub.status.idle": "2023-08-27T05:44:19.991291Z",
     "shell.execute_reply": "2023-08-27T05:44:19.990419Z",
     "shell.execute_reply.started": "2023-08-27T05:44:18.410277Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a subplot for the first 25 images\n",
    "fig, axes = plt.subplots(6, 6, figsize=(15, 16))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Get the i-th image\n",
    "    img = dataset[i]\n",
    "    # Rescale the image to [0, 1] for plotting\n",
    "    img_rescaled = (img + 1) / 2\n",
    "    # Plot the image on the i-th subplot\n",
    "    ax.imshow(img_rescaled)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Add a super title\n",
    "fig.suptitle('Original Dataset Preprocessed Images', fontsize=25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500d97e",
   "metadata": {},
   "source": [
    "<a id=\"architecture\"></a>\n",
    "# <p style=\"background-color:#22199e; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 3 | Define DCGAN Architecture</p>\n",
    "\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e9c82",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #d5d5ed; font-size:115%; text-align:left\">\n",
    "    \n",
    "In this part, we define the architecture for the __Discriminator__, __Generator__, and the __combined GAN model__:\n",
    "    \n",
    "* The __Discriminator__ is a Convolutional Neural Network (CNN) that classifies whether an image is real or fake (generated). \n",
    " \n",
    "* The __Generator__ is a CNN that upsamples a random noise vector into an image (fake image). \n",
    " \n",
    "* The __combined GAN model__, where the Discriminator's trainability is frozen, is used for training the Generator. It takes noise as input and the Discriminator classifies the generated images. In this setup, the Generator learns to create images that the Discriminator is unable to distinguish from real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7c0de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:44:19.994374Z",
     "iopub.status.busy": "2023-08-27T05:44:19.993484Z",
     "iopub.status.idle": "2023-08-27T05:44:22.458337Z",
     "shell.execute_reply": "2023-08-27T05:44:22.457568Z",
     "shell.execute_reply.started": "2023-08-27T05:44:19.994338Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_discriminator(image_shape=(64, 64, 3)):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Initial convolutional layer\n",
    "    model.add(Conv2D(128, (3, 3), strides=(2,2), padding='same', input_shape=image_shape))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    model.add(Conv2D(128, (3, 3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Third convolutional layer\n",
    "    model.add(Conv2D(256, (3, 3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Fourth convolutional layer\n",
    "    model.add(Conv2D(256, (3, 3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Fifth convolutional layer\n",
    "    model.add(Conv2D(512, (3, 3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    # Flatten and dense layer for classification\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Define optimizer and compile model\n",
    "    optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build and display discriminator summary\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7aaacf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:44:22.459941Z",
     "iopub.status.busy": "2023-08-27T05:44:22.459386Z",
     "iopub.status.idle": "2023-08-27T05:44:22.647465Z",
     "shell.execute_reply": "2023-08-27T05:44:22.646695Z",
     "shell.execute_reply.started": "2023-08-27T05:44:22.459905Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, channels=3):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Initial dense layer\n",
    "    model.add(Dense(16 * 16 * 128, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Reshape to (16, 16, 128) tensor for convolutional layers\n",
    "    model.add(Reshape((16, 16, 128)))\n",
    "    \n",
    "    # First deconvolutional layer\n",
    "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Second deconvolutional layer\n",
    "    model.add(Conv2DTranspose(128, (4, 4), strides=(1, 1), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))  \n",
    "    \n",
    "    # Third deconvolutional layer\n",
    "    model.add(Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))  \n",
    "    \n",
    "    # Fourth deconvolutional layer\n",
    "    model.add(Conv2DTranspose(64, (4, 4), strides=(1, 1), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))  \n",
    "    \n",
    "    # Output convolutional layer with 'tanh' activation\n",
    "    model.add(Conv2D(channels, (8, 8), activation='tanh', padding='same'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and display generator summary\n",
    "generator = build_generator(100)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44734eb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:44:22.648859Z",
     "iopub.status.busy": "2023-08-27T05:44:22.648508Z",
     "iopub.status.idle": "2023-08-27T05:44:22.660359Z",
     "shell.execute_reply": "2023-08-27T05:44:22.659671Z",
     "shell.execute_reply.started": "2023-08-27T05:44:22.648809Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    \n",
    "    # Setting discriminator as non-trainable, so its weights won't update when training the GAN\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # Creating the GAN model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adding the generator\n",
    "    model.add(generator)\n",
    "    \n",
    "    # Adding the discriminator\n",
    "    model.add(discriminator)\n",
    "\n",
    "    # Compiling the GAN model\n",
    "    optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553874bb",
   "metadata": {},
   "source": [
    "<a id=\"data_preparation\"></a>\n",
    "# <p style=\"background-color:#22199e; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 4 | Prepare Data for Training DCGAN</p>\n",
    "\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb9ff72",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #d5d5ed; font-size:115%; text-align:left\">\n",
    "\n",
    "Afterward, I am going to define some functions help in creating training batches for the Discriminator and Generator:\n",
    "\n",
    "* `generate_real_samples` - Selects random real images from the dataset and labels them as __1__, indicating they're real.\n",
    "\n",
    "    \n",
    "* `generate_noise_samples` - Produces random noise vectors. These are the inputs to the Generator, which it uses to create fake images.\n",
    "\n",
    "    \n",
    "* `generate_fake_samples` - Uses the Generator to produce fake images from the noise vectors and labels them as __0__, indicating they're fake.\n",
    "\n",
    "Together, these functions ensure that the Discriminator is trained to distinguish between real and fake images, while the Generator tries to produce images that the Discriminator can't differentiate from real ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049433ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:44:22.661622Z",
     "iopub.status.busy": "2023-08-27T05:44:22.661323Z",
     "iopub.status.idle": "2023-08-27T05:44:22.667685Z",
     "shell.execute_reply": "2023-08-27T05:44:22.666763Z",
     "shell.execute_reply.started": "2023-08-27T05:44:22.661592Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, num_samples):\n",
    "    sample_indices = randint(0, dataset.shape[0], num_samples)\n",
    "    X = dataset[sample_indices]\n",
    "    y = ones((num_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da23436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:44:22.669058Z",
     "iopub.status.busy": "2023-08-27T05:44:22.668738Z",
     "iopub.status.idle": "2023-08-27T05:44:22.680615Z",
     "shell.execute_reply": "2023-08-27T05:44:22.679853Z",
     "shell.execute_reply.started": "2023-08-27T05:44:22.669027Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_noise_samples(num_samples, noise_dim):\n",
    "    X_noise = randn(noise_dim * num_samples)\n",
    "    X_noise = X_noise.reshape(num_samples, noise_dim)\n",
    "    return X_noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ea1dfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:44:22.682354Z",
     "iopub.status.busy": "2023-08-27T05:44:22.681733Z",
     "iopub.status.idle": "2023-08-27T05:44:22.695506Z",
     "shell.execute_reply": "2023-08-27T05:44:22.694624Z",
     "shell.execute_reply.started": "2023-08-27T05:44:22.682322Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, noise_dim, num_samples):\n",
    "    X_noise = generate_noise_samples(num_samples, noise_dim)\n",
    "    X = generator.predict(X_noise)\n",
    "    y = zeros((num_samples, 1 ))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e990cd",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "# <p style=\"background-color:#22199e; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Step 5 | Train DCGAN & Monitor Progress</p>\n",
    "\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a1188",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #d5d5ed; font-size:115%; text-align:left\">\n",
    "\n",
    "Next, I am going to handle the simultaneous training of the Generator and Discriminator in our GAN model:\n",
    "\n",
    "1. `generate_images` - This function creates images from noise using the current state of the Generator for a given epoch. The images created during training are saved and can be visualized later to provide a glimpse into the Generator's evolving performance.\n",
    "\n",
    "2. `display_saved_images` - After training concludes, this function visualizes the saved images, presenting the progression of the Generator's capability across the epochs.\n",
    "    \n",
    "3. `plot_generated_images` - This function generates and plots images from noise using the current state of the Generator. It is utilized to visually track the Generator's performance during epochs as indicated in the training __output verbose__.\n",
    "\n",
    "4. `train` - This function orchestrates the core GAN training process. It runs through all epochs, and within each epoch, iterates over all batches. For every batch:\n",
    "   > - It first generates a batch of real images and a batch of fake images (created by the Generator), tagging each set with the appropriate labels. The Discriminator is then trained on these batches, and both its loss and accuracy metrics are computed.\n",
    "   >\n",
    "   > - Subsequently, a batch of noise samples is created, with the aim of deceiving the Discriminator into accepting these as genuine. As the Generator strives to amplify the loss, it learns to produce increasingly authentic-looking images.\n",
    "\n",
    "The above process is iteratively executed until all epochs are finalized. The `plot_generated_images` function is used to visually track the Generator's progress in real-time during training, whereas the `display_saved_images` function is used at the end of training to showcase the stored images, shedding light on the Generator's refinement over the training epochs.\n",
    "\n",
    "This combined training approach allows the Generator and Discriminator to learn together. The Discriminator improves at distinguishing fake images, and the Generator becomes better at fooling the Discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e4d5679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:44:22.700873Z",
     "iopub.status.busy": "2023-08-27T05:44:22.699610Z",
     "iopub.status.idle": "2023-08-27T05:44:22.708922Z",
     "shell.execute_reply": "2023-08-27T05:44:22.707944Z",
     "shell.execute_reply.started": "2023-08-27T05:44:22.700665Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_images(epoch, generator, num_samples=6, noise_dim=100):\n",
    "    \"\"\"\n",
    "    Generate images from the generator model for a given epoch.\n",
    "    \"\"\"\n",
    "    # Generate noise samples\n",
    "    X_noise = generate_noise_samples(num_samples, noise_dim)\n",
    "    \n",
    "    # Use generator to produce images from noise\n",
    "    X = generator.predict(X_noise, verbose=0)\n",
    "\n",
    "    # Rescale images to [0, 1] for visualization\n",
    "    X = (X + 1) / 2\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e98ca54e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:44:22.711045Z",
     "iopub.status.busy": "2023-08-27T05:44:22.710319Z",
     "iopub.status.idle": "2023-08-27T05:44:22.721564Z",
     "shell.execute_reply": "2023-08-27T05:44:22.720899Z",
     "shell.execute_reply.started": "2023-08-27T05:44:22.711012Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_saved_images(saved_images, display_frequency):\n",
    "    \"\"\"\n",
    "    Display the saved generated images after training.\n",
    "    \"\"\"\n",
    "    for epoch, images in enumerate(saved_images):\n",
    "        fig, axes = plt.subplots(1, len(images), figsize=(15, 3))\n",
    "        for i, img in enumerate(images):\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "        fig.suptitle(f\"Generated Images at Epoch {epoch*display_frequency + 1}\", fontsize=22)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d5e297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:44:22.723953Z",
     "iopub.status.busy": "2023-08-27T05:44:22.722940Z",
     "iopub.status.idle": "2023-08-27T05:44:22.732301Z",
     "shell.execute_reply": "2023-08-27T05:44:22.731603Z",
     "shell.execute_reply.started": "2023-08-27T05:44:22.723919Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_generated_images(epoch, generator, num_samples=6, noise_dim=100, figsize=(15, 3)):\n",
    "    \"\"\"\n",
    "    Plot and visualize generated images from the generator model for a given epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate noise samples\n",
    "    X_noise = generate_noise_samples(num_samples, noise_dim)\n",
    "    \n",
    "    # Use generator to produce images from noise\n",
    "    X = generator.predict(X_noise, verbose=0)\n",
    "\n",
    "    # Rescale images to [0, 1] for visualization\n",
    "    X = (X + 1) / 2\n",
    "\n",
    "    # Plotting the images\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=figsize)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        axes[i].imshow(X[i])\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Add a descriptive title\n",
    "    fig.suptitle(f\"Generated Images at Epoch {epoch+1}\", fontsize=22)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12e1bdc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T05:44:22.734742Z",
     "iopub.status.busy": "2023-08-27T05:44:22.733787Z",
     "iopub.status.idle": "2023-08-27T05:44:22.747992Z",
     "shell.execute_reply": "2023-08-27T05:44:22.747255Z",
     "shell.execute_reply.started": "2023-08-27T05:44:22.734708Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(generator_model, discriminator_model, gan_model, dataset, noise_dimension,\n",
    "          num_epochs=100, batch_size=128, display_frequency=10, verbose=1):\n",
    "    \n",
    "    # Create an empty list to store generated images for each epoch\n",
    "    saved_images_for_epochs = []\n",
    "    \n",
    "    # Calculate the number of batches per epoch\n",
    "    batches_per_epoch = int(dataset.shape[0] / batch_size)\n",
    "    \n",
    "    # Calculate half the size of a batch\n",
    "    half_batch_size   = int(batch_size / 2)\n",
    "\n",
    "    # Loop over all epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Loop over all batches within this epoch\n",
    "        for batch_num in range(batches_per_epoch):\n",
    "            \n",
    "            # Generate a batch of real images and their corresponding labels\n",
    "            real_images, real_labels    = generate_real_samples(dataset, half_batch_size)\n",
    "            # Train the discriminator on the real images and calculate loss and accuracy\n",
    "            dsr_loss_real, dsr_acc_real = discriminator_model.train_on_batch(real_images, real_labels)\n",
    "\n",
    "            # Generate a batch of fake images and their corresponding labels\n",
    "            fake_images, fake_labels    = generate_fake_samples(generator_model, noise_dimension, half_batch_size)\n",
    "            # Train the discriminator on the fake images and calculate loss and accuracy\n",
    "            dsr_loss_fake, dsr_acc_fake = discriminator_model.train_on_batch(fake_images, fake_labels)\n",
    "            \n",
    "            # Calculate the average discriminator loss and accuracy over real and fake images\n",
    "            dsr_loss = 0.5 * np.add(dsr_loss_real, dsr_loss_fake)\n",
    "            dsr_acc  = 0.5 * np.add(dsr_acc_real, dsr_acc_fake)\n",
    "            \n",
    "            # Generate noise samples and their corresponding labels for training the generator\n",
    "            gan_noise  = generate_noise_samples(batch_size, noise_dimension)\n",
    "            gan_labels = np.ones((batch_size, 1))\n",
    "            \n",
    "            # Train the generator and calculate loss\n",
    "            gen_loss, _ = gan_model.train_on_batch(gan_noise, gan_labels)\n",
    "            \n",
    "            if verbose:  # This condition checks if verbose is non-zero\n",
    "                # Print training information for this batch\n",
    "                print(f\"[ Epoch: {epoch+1} , Batch: {batch_num+1} ] --> [ Discriminator Loss : {dsr_loss:.6f} , Discriminator Accuracy: {100*dsr_acc:.2f}% ] [ Generator Loss: {gen_loss:.6f} ]\")\n",
    "                     \n",
    "        # Display generated images at the specified frequency\n",
    "        if epoch % display_frequency == 0:\n",
    "            generated_images_for_epoch = generate_images(epoch, generator_model)\n",
    "            saved_images_for_epochs.append(generated_images_for_epoch)\n",
    "            \n",
    "            # Plot generated images to visualize the progress of the generator\n",
    "            plot_generated_images(epoch, generator_model)\n",
    "\n",
    "    # Due to constraints on Kaggle output file size, saving the model is commented out.\n",
    "    # generator_model.save('Photorealistic_Face_Generator.h5')\n",
    "    \n",
    "    return saved_images_for_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f5a7ef0",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-27T05:44:22.751103Z",
     "iopub.status.busy": "2023-08-27T05:44:22.750567Z",
     "iopub.status.idle": "2023-08-27T08:28:02.267800Z",
     "shell.execute_reply": "2023-08-27T08:28:02.266790Z",
     "shell.execute_reply.started": "2023-08-27T05:44:22.751063Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_discriminator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17428/1342414833.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Build discriminator model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdiscriminator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Build generator model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_discriminator' is not defined"
     ]
    }
   ],
   "source": [
    "# Set noise dimension for generator input\n",
    "noise_dimension = 100\n",
    "\n",
    "# Build discriminator model\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Build generator model\n",
    "generator = build_generator(noise_dimension)\n",
    "\n",
    "# Combine generator and discriminator to form the GAN model\n",
    "gan_model = build_gan(generator, discriminator)\n",
    "\n",
    "# Train the GAN model on the dataset and get the saved images list\n",
    "saved_images = train(generator, discriminator, gan_model, dataset, noise_dimension, num_epochs=251, batch_size=128, display_frequency=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef021b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:28:02.273548Z",
     "iopub.status.busy": "2023-08-27T08:28:02.272514Z",
     "iopub.status.idle": "2023-08-27T08:28:04.917075Z",
     "shell.execute_reply": "2023-08-27T08:28:04.916075Z",
     "shell.execute_reply.started": "2023-08-27T08:28:02.273500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display all the saved images during \n",
    "display_saved_images(saved_images, display_frequency=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8c3e0",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:12px; padding: 20px; background-color: #d5d5ed; font-size:120%; text-align:center\">\n",
    "    \n",
    "Over __250 epochs__ of training, our GAN model has consistently evolved and refined its capabilities. Throughout these epochs, the generator's proficiency in producing lifelike faces has become increasingly evident in the images we've observed. However, due to the intensive demands of GAN training, constraints on our resources, and the limitations of our current model and dataset, there's still room for further enhancement. __Utilizing deeper models, a larger training dataset, and a higher number of epochs could potentially lead to even more realistic and refined results.__\n",
    "\n",
    "At this juncture, I invite you to experience the generator's current capabilities by viewing some of its creations. The progress achieved after 250 epochs is captivating to behold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ec8d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:28:04.919109Z",
     "iopub.status.busy": "2023-08-27T08:28:04.918532Z",
     "iopub.status.idle": "2023-08-27T08:28:09.654355Z",
     "shell.execute_reply": "2023-08-27T08:28:09.650395Z",
     "shell.execute_reply.started": "2023-08-27T08:28:04.919075Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_generated_images_after_training(generator, noise_dim=100, figsize=(15, 16)):\n",
    "    \n",
    "    fig, axes = plt.subplots(6, 6, figsize=figsize)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Generate noise samples\n",
    "        X_noise = generate_noise_samples(1, noise_dim)\n",
    "        \n",
    "        # Use generator to create an image\n",
    "        X = generator.predict(X_noise, verbose=0)\n",
    "        \n",
    "        # Rescale images to [0, 1] for plotting\n",
    "        X = (X + 1) / 2\n",
    "        \n",
    "        # Plot the image on the i-th subplot\n",
    "        ax.imshow(X[0])\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Add a super title\n",
    "    fig.suptitle('Generated Images after Training for 250 Epochs', fontsize=25)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_generated_images_after_training(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7701609f",
   "metadata": {},
   "source": [
    "<h2 align=\"left\"><font color='#22199e'>Best Regards!</font></h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
